{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f599db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import nearl\n",
    "import nearl.data\n",
    "\n",
    "table = pd.read_csv(nearl.data.REFINED_SET)\n",
    "print(table)\n",
    "\n",
    "plt.plot(table.pK+0.3)\n",
    "plt.plot(table.pK)\n",
    "plt.plot(table.pK-0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ca99a",
   "metadata": {},
   "source": [
    "# Pre-process and load the dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75328cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "# from BetaPose import utils\n",
    "\n",
    "\n",
    "def loaddb(dbfile):\n",
    "  with open(dbfile, \"r\") as file1: \n",
    "    lines = [i.strip() for i in file1.read().split(\"\\n\")]\n",
    "    lines = [i for i in lines if len(i)>0 and i[0]!=\"#\"]\n",
    "    lines = [re.sub(r\"//.*\", \"\", i).strip() for i in lines]\n",
    "    table = [i.split() for i in lines]\n",
    "  table = pd.DataFrame(table, columns = [\"PDB\",\"resolution\",\"year\",\"affinity\",\"kd/ki\"]);\n",
    "  table['year'] = table['year'].astype(int)\n",
    "  table['resolution'] = table['resolution'].astype(float)\n",
    "  table['affinity'] = table['affinity'].astype(float)\n",
    "  return table\n",
    "\n",
    "  \n",
    "# Pre-process the PDBBind-refined dataset (CSV)\n",
    "PB_setfile = \"/home/miemie/Dropbox/PhD/project_MD_ML/PDBbind_v2020_refined/index/INDEX_refined_data.2020\"\n",
    "table = loaddb(PB_setfile)\n",
    "a = list(table.PDB)\n",
    "print(f\"All Entries: {len(a)} ; Unique Entries: {len(set(a))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(table.year.astype(int), bins=10)\n",
    "plt.hist(table.affinity.astype(float), bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177b03f",
   "metadata": {},
   "source": [
    "# Only keep the selected entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# Marchand, Jean-RÃ©my, et al. \"An ABSINTH-Based Protocol for Predicting Binding Affinities between Proteins and Small Molecules.\" Journal of Chemical Information and Modeling 60.10 (2020): 5188-5202.\n",
    "# 754 entries\n",
    "\n",
    "abs_sel = ['1a4k','1a4w','1a69','1a8i','1a99','1aaq','1adl','1ai4','1ai5','1ai7','1aid','1ajn','1ajp','1ajq','1ajv','1ajx','1apw','1ax0','1axz','1b05','1b0h','1b1h','1b40','1b46','1b6h','1b6k','1b7h','1b9j','1bcu','1bdq','1bgq','1bhx','1bju','1bjv','1bq4','1br6','1bv7','1bv9','1bwa','1c3x','1c5n','1c5o','1c5p','1c5q','1c5s','1c5t','1c5x','1c5y','1c5z','1c70','1c83','1c84','1c86','1c87','1c88','1ce5','1cea','1ceb','1d4h','1d4i','1d4k','1d4l','1d6v','1d7i','1d7j','1db1','1df8','1dfo','1dhi','1dhj','1dmp','1drk','1dzk','1e1v','1e1x','1e2k','1e2l','1e3v','1e6s','1eb2','1ebw','1eby','1ebz','1ec0','1ec1','1ecv','1efy','1ejn','1ela','1eld','1ele','1enu','1epo','1erb','1ezq','1f0r','1f0u','1f4e','1f4g','1f4x','1fcx','1fcy','1fcz','1fd0','1fh7','1fh8','1fh9','1fhd','1fj4','1fkg','1fkh','1fki','1fl3','1fpc','1g2k','1g2l','1g30','1g32','1g35','1g36','1g74','1g7g','1g85','1gcz','1ghv','1ghw','1ghz','1gi1','1gi7','1gj6','1gja','1gni','1gpn','1gu1','1gyx','1gyy','1gzc','1h1p','1h1s','1h22','1h23','1h9z','1ha2','1hbv','1hmr','1hms','1hmt','1hps','1hpv','1hpx','1hsh','1hvh','1hvi','1hvj','1hvk','1hvl','1hvr','1hvs','1hwr','1hxb','1hxw','1i00','1i5r','1igj','1ii5','1izh','1izi','1j14','1j16','1j17','1j4r','1jak','1jet','1jgl','1jqy','1jsv','1jwt','1jys','1jzs','1k1i','1k1j','1k1l','1k1m','1k1n','1k21','1k22','1k4g','1k4h','1kdk','1kv1','1kyv','1kzk','1kzn','1l2s','1l83','1laf','1lag','1lah','1lbk','1lee','1lf2','1lgw','1li2','1li3','1li6','1lke','1lnm','1lpg','1lpk','1lpz','1lst','1m2q','1m2r','1m48','1mes','1met','1mfi','1mq5','1mq6','1mrw','1mrx','1msm','1msn','1mtr','1mu6','1mu8','1n1m','1n1t','1n2v','1n46','1n4h','1n7m','1n8v','1nc1','1nc3','1nf8','1nfu','1nfw','1nfy','1nhu','1nl9','1nli','1nny','1no6','1nq7','1nt1','1nvq','1nvr','1nvs','1nw7','1nz7','1o2o','1o2q','1o2s','1o2w','1o2x','1o2z','1o30','1o33','1o36','1o3d','1o3i','1o3j','1o3k','1o3p','1o5a','1o5b','1o5c','1o5e','1o5g','1oba','1ocq','1od8','1odi','1odj','1ogd','1ogz','1ohr','1ony','1onz','1os5','1oss','1owe','1owh','1oyq','1oyt','1p1n','1p1o','1p1q','1p57','1pb8','1pb9','1pbq','1pot','1ppc','1pph','1pr1','1pxn','1pxp','1pzi','1q63','1q65','1q66','1q72','1q8t','1qan','1qaw','1qb1','1qbn','1qbo','1qbr','1qbs','1qbu','1qbv','1qi0','1qiw','1qy1','1qy2','1r0p','1r4w','1r5y','1r6n','1r9l','1rd4','1rpj','1s38','1sbg','1sdt','1sdu','1sdv','1sgu','1sh9','1siv','1sqo','1sr7','1srg','1ssq','1stc','1sv3','1sw2','1swg','1swr','1syh','1syi','1t4v','1t7j','1ta2','1ta6','1tcw','1tcx','1td7','1tng','1tnh','1tni','1tog','1toi','1toj','1tok','1tom','1u1w','1ugw','1ugx','1uou','1upf','1urg','1usi','1usk','1utj','1utl','1utm','1utn','1uv6','1uvt','1uw6','1uwf','1uz4','1v0k','1v0l','1v1j','1v2j','1v2k','1v2l','1v2n','1v2o','1v2q','1v2r','1v2s','1v2t','1v2u','1v2w','1vfn','1vj9','1vja','1vyf','1vyg','1vyq','1vzq','1w0z','1w11','1w13','1w3j','1w3k','1w5v','1w5w','1w5x','1w5y','1wcq','1wdn','1we2','1wht','1wm1','1ws4','1ws5','1x8j','1x9d','1xap','1xff','1xgi','1xgj','1xhy','1xk9','1xka','1xkk','1xow','1xt8','1y1z','1y20','1y3n','1y6q','1y6r','1yc1','1yc4','1ydk','1ydr','1yds','1ydt','1yqj','1z1r','1z6e','1zgi','1zhy','1zoe','1zog','1zp8','220l','2a4m','2a8g','2aac','2aj8','2aqu','2avo','2avs','2avv','2ayr','2azr','2b07','2b1v','2b4l','2bak','2bal','2boh','2bok','2bpv','2bpy','2bq7','2bqv','2br1','2brb','2brm','2bt9','2bvd','2bvr','2bvs','2byr','2bys','2bz6','2bza','2c1p','2c3l','2cbu','2cej','2cen','2cgf','2cht','2cji','2csn','2d0k','2d3u','2d3z','2drc','2dri','2e2r','2e7f','2epn','2exm','2f1g','2f2h','2f34','2f35','2f5t','2f80','2f81','2f8g','2fgu','2fgv','2flr','2fpz','2fqw','2fqx','2fr3','2fw6','2fwp','2fx6','2fxu','2g79','2g8r','2g94','2gl0','2glp','2gss','2gst','2gv6','2gv7','2h4g','2h4k','2h6b','2ha2','2ha3','2ha5','2ha6','2hb1','2hb3','2hhn','2hjb','2hs1','2hs2','2hxm','2i0a','2i2b','2i4j','2i4u','2i4v','2i80','2idw','2ien','2ieo','2ihq','2iko','2il2','2iuz','2iwx','2izl','2j2u','2j34','2j47','2j4g','2j4i','2j77','2j7e','2j7g','2j7h','2j9n','2jds','2jfz','2jh0','2jh5','2jh6','2jiw','2nmy','2nmz','2nnk','2nnp','2nt7','2nta','2o0u','2o2u','2o4j','2o4k','2o4r','2o4s','2oag','2ogy','2ojg','2ojj','2ok1','2on6','2oxd','2oxx','2oxy','2oyk','2oym','2p16','2p4j','2p4y','2p7a','2p7z','2p95','2pcp','2pgz','2pk5','2pk6','2pql','2pqz','2psu','2psv','2pu2','2pvh','2pvj','2pvk','2pvl','2pwc','2pwd','2pwg','2pwr','2pyn','2q54','2q55','2q5k','2q63','2q64','2q88','2q89','2qbq','2qbr','2qbs','2qbu','2qci','2qd6','2qd7','2qd8','2qe4','2qfo','2qg0','2qg2','2qhm','2qhy','2qhz','2qi0','2qi1','2qi3','2qi4','2qi5','2qi6','2qi7','2qm9','2qmg','2qnn','2qnq','2qrl','2qt5','2qtg','2qu6','2r2m','2r2w','2r38','2r3t','2r3w','2r43','2r5a','2r5p','2r6w','2r6y','2ra0','2rcb','2ri9','2rkf','2rkg','2sim','2std','2tpi','2uwo','2uxz','2uy0','2v00','2v3u','2v95','2vh6','2vkm','2vnt','2vw5','2vyt','2z4b','2zb0','2zb1','3aid','3b4p','3b50','3b5r','3b65','3b66','3b67','3b68','3b7j','3be9','3bex','3bfu','3bgb','3bgc','3bgq','3bgz','3bra','3brn','3bu1','3buf','3bug','3buh','3bvb','3c2u','3cct','3ccw','3ccz','3cd0','3cd5','3cd7','3cda','3cdb','3cf8','3cj2','3cj4','3cj5','3ckp','3cs7','3cyw','3cyx','3d0b','3d1x','3d1y','3d1z','3d20','3d7z','3d83','3d94','3djk','3e5a','3e5u','3e64','3e92','3e93','3ebl','3eko','3ekr','3eqr','3f8c','3f8f','3gss','3gst','3jdw','3kiv','4ts1','5er1','5std','5yas','6std','7std']\n",
    "abs_sel = [i.upper() for i in abs_sel]; \n",
    "abs_sel = list(set(abs_sel)); \n",
    "print(f\"ABSINTH Entries: {len(abs_sel)} ; ABSINTH Unique: {len(set(abs_sel))}\"); \n",
    "\n",
    "table_abs = pd.DataFrame(columns = [\"PDB\",\"resolution\",\"year\",\"affinity\",\"kd/ki\"])\n",
    "for i in abs_sel:\n",
    "  theline = table.loc[table.PDB == i.lower()]\n",
    "  if len(theline) > 0:\n",
    "    table_abs = pd.concat([table_abs,theline],ignore_index=True)\n",
    "\n",
    "table_abs = table_abs.reset_index()\n",
    "table_abs = table_abs.set_index(\"index\")\n",
    "table_abs\n",
    "# print(len(table_abs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0ca23",
   "metadata": {},
   "source": [
    "# Calculate the Ligand Embeding Factor / Heavy Atom Number / Protein-ligand distance / PDB titles / Portein Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04d161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from BetaPose import utils\n",
    "\n",
    "basepath = \"/home/miemie/Dropbox/PhD/project_MD_ML/PDBbind_v2020_refined/\"\n",
    "csvfile = \"/home/miemie/Dropbox/PhD/project_MD_ML/PDBbind_v2020_refined/index/INDEX_ABS_SEL.csv\"; \n",
    "\n",
    "thetable = pd.read_csv(csvfile, index_col=0); \n",
    "\n",
    "emb_factor = []; \n",
    "c_nha = []; \n",
    "l_nha = []; \n",
    "dist_before = []; \n",
    "titles = []; \n",
    "seqs = []; \n",
    "\n",
    "for idx,row in thetable.iterrows():\n",
    "#   if idx == 5:\n",
    "#     break\n",
    "  pdbcode = row.PDB; \n",
    "  embfactor = utils.EmbeddingFactor(basepath, pdbcode); \n",
    "  \n",
    "  print(f\"Processing the entry {idx} - {pdbcode}; \")\n",
    "  \n",
    "  # Collect needed info\n",
    "  complex_file = f\"{basepath}/{pdbcode}/{pdbcode}_complex.pdb\"\n",
    "  complex_nha = utils.PRO_nha(complex_file); \n",
    "  lig_nha = utils.LIG_nha(complex_file)\n",
    "  dist_b = utils.DistanceLigPro(complex_file, mode=\"file\", ligname=\"LIG\")\n",
    "  title = utils.getPdbTitle(pdbcode); \n",
    "  seq = utils.getPdbSeq(pdbcode); \n",
    "  \n",
    "  # Append new value to lists\n",
    "  dist_before.append(dist_b);\n",
    "  emb_factor.append(embfactor);\n",
    "  c_nha.append(complex_nha); \n",
    "  l_nha.append(lig_nha);\n",
    "  titles.append(title); \n",
    "  seqs.append(seq); \n",
    "#   print(pdbcode, embfactor, complex_nha, lig_nha)\n",
    "\n",
    "if len(emb_factor) == len(thetable) :\n",
    "  thetable[\"embedding_factor\"] = emb_factor; \n",
    "else: \n",
    "  print(f\"Failed to match the length of the table {len(thetable)} and embedding_factor {len(emb_factor)}\")\n",
    "  \n",
    "if len(c_nha) == len(thetable):\n",
    "  thetable[\"complex_nha\"] = c_nha; \n",
    "else: \n",
    "  print(f\"Failed to match the length of the table {len(thetable)} and complex_nha {len(c_nha)}\")\n",
    "  \n",
    "if len(l_nha) == len(thetable):\n",
    "  thetable[\"lig_nha\"] = l_nha; \n",
    "else: \n",
    "  print(f\"Failed to match the length of the table {len(thetable)} and lig_nha {len(l_nha)}\")\n",
    "  \n",
    "if len(dist_before) == len(thetable):\n",
    "  thetable[\"distB\"] = dist_before; \n",
    "else: \n",
    "  print(f\"Failed to match the length of the table {len(thetable)} and dist_before {len(dist_before)}\")\n",
    "  \n",
    "if len(titles) == len(thetable):\n",
    "  thetable[\"title\"] = titles; \n",
    "else: \n",
    "  print(f\"Failed to match the length of the table {len(thetable)} and titles {len(titles)}\")\n",
    "  \n",
    "if len(seqs) == len(thetable):\n",
    "  thetable[\"seq\"] = seqs; \n",
    "else: \n",
    "  print(f\"Failed to match the length of the table {len(thetable)} and seq {len(seqs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95645b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSSEL_setfile = \"/home/miemie/Dropbox/Documents/BetaPose/INDEX_SEL_DATA.csv\"\n",
    "# titles = thetable.seq\n",
    "# seqs = thetable.title\n",
    "# thetable[\"seq\"] = seqs\n",
    "# thetable[\"title\"] = titles\n",
    "# with open(ABSSEL_setfile, 'w') as file1: \n",
    "#   file1.write(thetable.to_csv())\n",
    "# thetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053777a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "data = thetable.embedding_factor\n",
    "print(f\"Embedding Factor: Mean: {data.mean()} ; Std: {data.std().round(3)} ; Threshold {round(data.mean() - data.std(),2)}\"); \n",
    "data = thetable.complex_nha\n",
    "print(f\"Complex NHA: Mean: {data.mean()} ; Std: {data.std()} ; Threshold {round(data.mean() + data.std())}\"); \n",
    "data = thetable.lig_nha\n",
    "print(f\"Ligand NHA: Mean: {data.mean()} ; Std: {data.std()} ; Threshold {round(data.mean() - data.std())}\"); \n",
    "\n",
    "data = thetable.embedding_factor\n",
    "threshold = 0.6\n",
    "print(f\"Embedding Factor greater than {threshold}: {np.count_nonzero(data > threshold)}/{len(thetable)}\")\n",
    "threshold = 0.7\n",
    "print(f\"Embedding Factor greater than {threshold}: {np.count_nonzero(data > threshold)}/{len(thetable)}\")\n",
    "threshold = 0.8\n",
    "print(f\"Embedding Factor greater than {threshold}: {np.count_nonzero(data > threshold)}/{len(thetable)}\")\n",
    "# nha_threshold = 6035\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db910f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24dc24e",
   "metadata": {},
   "source": [
    "# Calculate the heavy/hydrogen atom number ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1a32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratios=[];\n",
    "for i,row in table_abs.iterrows():\n",
    "  pdbcode= row.PDB.lower();\n",
    "  pdbfile = f\"/home/miemie/Dropbox/PhD/project_MD_ML/PDBbind_v2020_refined/{pdbcode}/{pdbcode}_complex.pdb\";\n",
    "  nha = utils.PRO_nha(pdbfile); \n",
    "  nh  = utils.PRO_nhydrogen(pdbfile); \n",
    "  ratios.append(nha/nh); \n",
    "#   print(f\"{nha}, {nh}, ratio: {nha/nh}\")\n",
    "print(np.mean(ratios)) \n",
    "# Average ration of nha and hydrogen number: 1.109888041096362\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b97d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf1f9a7",
   "metadata": {},
   "source": [
    "<h3 style='color:#b22222'>Problems</h3>\n",
    "\n",
    "- C4001N46 Ligand left the pocket \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ef0cc",
   "metadata": {},
   "source": [
    "# Title Sentence Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Load model\n",
    "modelfile= \"/home/miemie/Downloads/wiki_unigrams.bin\"\n",
    "model = cluster.LoadModel(modelfile); \n",
    "\n",
    "# Embed the sentences to finite feature space. \n",
    "vector = cluster.SentenceEmbed(thetable.title); \n",
    "\n",
    "# Determine the best cluster number\n",
    "func = cluster.agglomerative; \n",
    "iterations = range(10,90,3); \n",
    "clusternr = cluster.OptimalCluster(vector, func, iterations, method=\"db\")\n",
    "\n",
    "clusters = func(vector, clusternr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65345d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a24e18a",
   "metadata": {},
   "source": [
    "# Protein Sequence Embeding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf10029",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "\n",
    "def view2d(data, labels):\n",
    "  pca = PCA(n_components=2)\n",
    "  reduced = pca.fit_transform(data)\n",
    "  fig = plt.scatter(reduced[:,0], reduced[:,1], c=labels)\n",
    "  return fig\n",
    "  \n",
    "labels = cluster.kmeans(embed_df,10)\n",
    "view2d(embed_df, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb305e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from BetaPose import cluster\n",
    "import seaborn as sns; sns.set(); \n",
    "\n",
    "np.random.seed(115); \n",
    "\n",
    "title_csv = \"/home/miemie/Dropbox/Documents/BetaPose/INDEX_SEL_DATA.csv\"; \n",
    "# title_csv = \"/home/yzhang/Documents/Personal_documents/BetaPose/test110.csv\"\n",
    "thetable = pd.read_csv(title_csv, index_col=0); \n",
    "\n",
    "embed_df = cluster.SequenceEmbed(thetable.seq); \n",
    "\n",
    "func = cluster.agglomerative; \n",
    "# func = cluster.kmeans; \n",
    "iterations = range(10,90,1); \n",
    "clusternr = cluster.OptimalCluster(embed_df, func, iterations, method=\"db\")\n",
    "\n",
    "clusters = func(embed_df, clusternr)\n",
    "choice = cluster.RandomPerCluster(clusters, 3)\n",
    "choice = np.random.choice(choice, 200, replace=False)\n",
    "choice.sort()\n",
    "\n",
    "counts = np.unique(clusters, return_counts=True)\n",
    "c = 1\n",
    "for i,j in zip(*counts): \n",
    "  if (c) % 5 == 0: \n",
    "    print(f\"{i:>4} , {j:<4}\")\n",
    "  else: \n",
    "    print(f\"{i:>4} , {j:<4} | \", end=\"\")\n",
    "  c += 1\n",
    "\n",
    "# theslice = thetable[[\"affinity\", \"embedding_factor\", \"complex_nha\", \"lig_nha\", \"TMScore\", \"distA\"]]\n",
    "theslice = thetable[[\"affinity\", \"embedding_factor\", \"distB\",\"complex_nha\", \"lig_nha\"]]\n",
    "sns.pairplot(theslice, size=1.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b6ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d42fa",
   "metadata": {},
   "source": [
    "# Select several lines with the following criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43eb79d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "rows_old = len(choice)\n",
    "rows = thetable.loc[choice]; \n",
    "rows = rows.reset_index(drop=1); \n",
    "\n",
    "status = (rows.lig_nha > 12) * (rows.lig_nha < 40) * (rows.complex_nha < 6035); \n",
    "\n",
    "rows = rows.loc[status]; \n",
    "rows = rows.reset_index(drop=1);  \n",
    "\n",
    "print(f\"Entries kept {np.count_nonzero(status)}/{rows_old} after filtration; After unique: {len(rows.PDB)}/{len(set(rows.PDB))}\"); \n",
    "\n",
    "# rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4167e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.scatter(rows.embedding_factor, rows.affinity)\n",
    "outcsv = \"/home/miemie/Dropbox/Documents/BetaPose/test142.csv\"\n",
    "with open(outcsv, \"w\") as file1: \n",
    "  file1.write(rows.to_csv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484d8c0",
   "metadata": {},
   "source": [
    "# PDB fix by PDBFixer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f150fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdbfixer import PDBFixer; \n",
    "from openmm.app import PDBFile; \n",
    "import os.path, re; \n",
    "import pytraj as pt \n",
    "import numpy as np \n",
    "from scipy.spatial import distance_matrix\n",
    "import tempfile\n",
    "\n",
    "# PDBFixer(\"4kng\")\n",
    "pdbcode = \"1O5E\"\n",
    "pdbfile = f\"{PDBBind_path}/{pdbcode.lower()}/{pdbcode.lower()}_protein.pdb\"; \n",
    "print(pdbfile)\n",
    "\n",
    "\n",
    "\n",
    "proteinResidues = ['ALA', 'ASN', 'CYS', 'GLU', 'HIS', 'LEU', 'MET', 'PRO', 'THR', 'TYR', 'ARG', 'ASP', 'GLN', 'GLY', 'ILE', 'LYS', 'PHE', 'SER', 'TRP', 'VAL']\n",
    "if os.path.isfile(pdbfile):\n",
    "  with tempfile.NamedTemporaryFile(suffix=\".pdb\") as file1: \n",
    "    with open(pdbfile, \"r\") as file2: \n",
    "      retained_lines = [i for i in file2.read().strip(\"\\n\").split(\"\\n\") if (\"ATOM\" in i or \"HETATM\" in i) and i[26].strip() == \"\"]\n",
    "    with open(file1.name, \"w\") as file2: \n",
    "      file2.write(\"\\n\".join(retained_lines)); \n",
    "    fixer = PDBFixer(filename=file1.name); \n",
    "    fixer.findNonstandardResidues()\n",
    "    fixer.replaceNonstandardResidues()\n",
    "    fixer.removeHeterogens(False)\n",
    "    fixer.addMissingHydrogens(7.0)\n",
    "  \n",
    "  with tempfile.NamedTemporaryFile(suffix=\".pdb\") as file1: \n",
    "    PDBFile.writeFile(fixer.topology, fixer.positions, open(file1.name, 'w'))\n",
    "    traj = pt.load(file1.name); \n",
    "    \n",
    "    with open(pdbfile, \"r\") as file2: \n",
    "      pdblines = file2.read().strip(\"\\n\").split(\"\\n\"); \n",
    "      remarks = [i for i in pdblines if \"SEQRES\" in i or \"SSBOND\" in i or \"REMARK\" in i]; \n",
    "      remark_str = \"\\n\".join(remarks); \n",
    "    with open(file1.name, \"r\") as file2: \n",
    "      pdblines = file2.read().strip(\"\\n\").split(\"\\n\"); \n",
    "      chainids = [i[17:26] for i in pdblines if \"ATOM\" in i or \"HETATM\" in i]; \n",
    "      c = 0; \n",
    "      processed = []; \n",
    "      chainindexes = []; \n",
    "      for rec in chainids: \n",
    "        if rec not in processed:\n",
    "          processed.append(rec); \n",
    "          if len(rec[4].strip()) == 1:\n",
    "            chainindexes.append(rec[4].strip())\n",
    "          c += 1; \n",
    "      print(len(chainindexes), \" Residues found\", chainindexes)\n",
    "      chainids = chainindexes; \n",
    "  \n",
    "    # After the fix of the PDBfile, find clashes\n",
    "    traj.top.set_reference(traj[0]);\n",
    "    atoms = np.array([i for i in traj.top.atoms]); \n",
    "    residues = np.array([i for i in traj.top.residues]); \n",
    "    c_conflicts = 0; \n",
    "    excludes = [\"N-C\", \"C-N\", \"SG-SG\"]; \n",
    "    atoms_to_remove = []; \n",
    "    for res in traj.top.residues: \n",
    "      resmask = \"@\"+\",\".join([str(i) for i in range(res.first+1, res.last+1)])\n",
    "      # idx1 = traj.top.select(f\"{resmask}&!@H=,?H=\")\n",
    "      idx1 = traj.top.select(f\"{resmask}&!@H=,?H=\")\n",
    "      mtx1 = traj.xyz[0][idx1]; \n",
    "\n",
    "      # closest = traj.top.select(f\"{resmask}<:6&!{resmask}&!@H=,?H=\")\n",
    "      closest = traj.top.select(f\"{resmask}<:6&!{resmask}\")\n",
    "      mtx2 = traj.xyz[0][closest]; \n",
    "      dist_mtx = distance_matrix(mtx1, mtx2); \n",
    "      len_check = (dist_mtx < 1.6); \n",
    "\n",
    "      if np.count_nonzero(len_check) > 0:\n",
    "        # print(f\"Residue {res.name} {res.index} identified {np.count_nonzero(len_check)} conflicts\")\n",
    "        group1 = idx1[np.where(len_check)[0]]\n",
    "        group2 = closest[np.where(len_check)[1]]\n",
    "        partners = [f\"{atoms[i].name}-{atoms[j].name}\" for i, j in zip(group1, group2)]; \n",
    "        distinct_partners = [i for i in partners if i not in excludes]; \n",
    "        if len(distinct_partners) > 0: \n",
    "          for i, j in zip(group1, group2): \n",
    "            if f\"{atoms[i].name}-{atoms[j].name}\" not in excludes: \n",
    "              if residues[atoms[i].resid].name in proteinResidues: \n",
    "                lst1 = traj.top.select(f\":{atoms[i].resid+1}&!@C,H,N,O,CA,HA\"); \n",
    "              else: \n",
    "                lst1 = traj.top.select(f\":{atoms[i].resid+1}\"); \n",
    "              if residues[atoms[j].resid].name in proteinResidues: \n",
    "                lst2 = traj.top.select(f\":{atoms[j].resid+1}&!@C,H,N,O,CA,HA\"); \n",
    "              else: \n",
    "                lst2 = traj.top.select(f\":{atoms[j].resid+1}\"); \n",
    "              tmplst = list(lst1) + list(lst2); \n",
    "              atoms_to_remove += tmplst\n",
    "              c_conflicts += 1\n",
    "    atoms_to_remove = list(set(atoms_to_remove)); \n",
    "    atoms_to_remove.sort(); \n",
    "\n",
    "    if (len(atoms_to_remove)/len(atoms)) > 0.1: \n",
    "      print(f\"Warning: The atoms to remove ({len(atoms_to_remove)}) is over 10% of the total atom number {len(atoms_to_remove)/len(atoms)}\"); \n",
    "      print(\"Please be VERY CAUTIOUS about the structure or adjust the threshold.\"); \n",
    "    elif len(atoms_to_remove) > 100: \n",
    "      print(f\"Warning: The atoms to remove ({len(atoms_to_remove)}) is over 100\"); \n",
    "      print(\"Please be VERY CAUTIOUS about the structure or adjust the threshold.\"); \n",
    "    else: \n",
    "      print(f\"Atoms to remove: {len(atoms_to_remove)}\"); \n",
    "\n",
    "    ofile = f\"{PDBBind_path}/{pdbcode.lower()}/{pdbcode.lower()}_processed.pdb\";\n",
    "  #   pt.save(ofile, traj, overwrite=True)\n",
    "\n",
    "    with open(file1.name, \"r\") as file2: \n",
    "      finalstr = f\"\"; \n",
    "      c = 0; \n",
    "      pdblines = file2.read().strip(\"\\n\").split(\"\\n\")\n",
    "      for line in pdblines: \n",
    "        if re.match(\"^ATOM|^HETATM\", line):\n",
    "          if c in atoms_to_remove: \n",
    "            c += 1  \n",
    "            continue\n",
    "          else: \n",
    "            finalstr += f\"{line}\\n\"\n",
    "            c += 1  \n",
    "        else: \n",
    "          finalstr += f\"{line}\\n\"\n",
    "  #   finalstr = f\"{remark_str}\\n\"; \n",
    "  #   for idx, atom in enumerate(atoms): \n",
    "  #     if idx not in atoms_to_remove: \n",
    "  #       point = traj.xyz[0][idx]; \n",
    "  #       tmpstr = \"\".join([f\"{i:>8.3f}\" for i in point]); \n",
    "  #       print(atom.chain, atom.resname)\n",
    "  #       thisline = f\"ATOM  {idx+1:>5} {atom.name:^4} {residues[atom.resid].name} {chainids[atom.resid]}{atom.resid+1:>4d}    {tmpstr}{np.round(1.0,2):>6}{round(0.0,2):>6}\\n\"\n",
    "  #       finalstr += thisline; \n",
    "  #   print(finalstr)\n",
    "    with open(ofile, \"w\") as file1: \n",
    "      file1.write(finalstr+\"END\\n\")\n",
    "    print(f\"Conflict identified: {c_conflicts}\")\n",
    "    print(dir(fixer))\n",
    "  \n",
    "else:\n",
    "  print(\"No\")\n",
    "\n",
    "# for i in atoms:\n",
    "#   print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
