{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0227ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BetaPose import data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BetaPose import data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d37a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import h5py as h5 \n",
    "# import os\n",
    "\n",
    "\n",
    "matrix = np.arange(100*32*32*32).reshape((100,32,32,32))\n",
    "\n",
    "targetval = np.arange(100)\n",
    "\n",
    "# # print(matrix)\n",
    "with h5.File(\"/tmp/mytestfile.h5\", \"w\") as file1:\n",
    "  print(matrix.shape)\n",
    "  dset = file1.create_dataset(\"train_x\", matrix.shape, dtype='i4', data=matrix, compression=\"gzip\", maxshape=(None, 32,32,32))\n",
    "  targetvals = file1.create_dataset(\"train_y\", targetval.shape, dtype='i4', data=targetval, compression=\"gzip\", maxshape=(None))\n",
    "\n",
    "\n",
    "# HDF5 is 187 times smaller than pickle \n",
    "dt = h5.special_dtype(vlen=np.dtype(float))\n",
    "data = np.array([(1, 1.1, np.arange(100).reshape((10,10))), (2, 2.2, np.arange(100,200).reshape((10,10)))], \n",
    "                dtype=[('col1', 'i4'), ('col2', 'f4'), ('col3', dt)])\n",
    "\n",
    "\n",
    "class hdf_operator:\n",
    "  def __init__(self, filename, new=False): \n",
    "    if new or (not os.path.isfile(filename)): \n",
    "      self.hdffile = h5.File(filename, \"w\")\n",
    "    else: \n",
    "      self.hdffile = h5.File(filename, \"a\")\n",
    "    \n",
    "  def list_datasets(self):\n",
    "    \"\"\"\n",
    "    List all available datasets in the HDF5 file.\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    def find_datasets(name, obj):\n",
    "      if isinstance(obj, h5.Dataset):\n",
    "        datasets.append(name)\n",
    "    self.hdffile.visititems(find_datasets)\n",
    "    return datasets\n",
    "  \n",
    "  def data(self, key):\n",
    "    dset = self.hdffile[key]\n",
    "    return dset\n",
    "  def close(self):\n",
    "    self.hdffile.close()\n",
    "  \n",
    "  def column_names(self, dataset_name):\n",
    "    \"\"\"\n",
    "    Get the column names of a dataset.\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    Returns:\n",
    "    list of str: A list of column names for the specified dataset.\n",
    "    \"\"\"\n",
    "    dataset = self.data(dataset_name)\n",
    "    if isinstance(dataset.dtype, np.dtype):\n",
    "      return dataset.dtype.names\n",
    "    else:\n",
    "      raise ValueError(f\"{dataset_name} is not a structured dataset with column names.\")\n",
    "  \n",
    "  def mask_entries(self, dataset_name, boolean_mask):\n",
    "    \"\"\"\n",
    "    Delete a set of entries in a given dataset using a boolean array.\n",
    "    NOTE: might be slow when dealing with ultra large files\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    boolean_mask (array-like): A boolean array indicating which entries to keep.\n",
    "    \"\"\"\n",
    "    boolean_mask = np.bool_(boolean_mask)\n",
    "    if dataset_name not in self.list_datasets(): \n",
    "      raise Exception(f\"Not found any dataset named {dataset_name}\")\n",
    "      \n",
    "    # Retrieve the dataset\n",
    "    dataset = self.data(dataset_name)\n",
    "    shape_before = dataset.shape; \n",
    "    \n",
    "    # Create a new dataset without the specified entries\n",
    "    new_data = dataset[:][boolean_mask]\n",
    "    new_dataset = self.create_dataset(f\"{dataset_name}_temp\", new_data)\n",
    "    shape_after = new_dataset.shape; \n",
    "    # Copy attributes from the original dataset to the new one\n",
    "    for key, value in dataset.attrs.items():\n",
    "      new_dataset.attrs[key] = value\n",
    "\n",
    "    # Remove the original dataset and rename the new one\n",
    "    self.hdffile.pop(dataset_name, None);\n",
    "    self.hdffile.move(f\"{dataset_name}_temp\", dataset_name); \n",
    "    print(f\"Successfully masked {np.count_nonzero(boolean_mask)} entries; Shape: {shape_before} -> {shape_after}\")\n",
    "    \n",
    "  def remove_entry(self, dataset_name, index):\n",
    "    \"\"\"\n",
    "    Remove an entry from the specified dataset by index.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    index (int): The index of the entry to remove.\n",
    "    \"\"\"\n",
    "    dataset = self.data(dataset_name)\n",
    "    shape_before = dataset.shape; \n",
    "    data = np.asarray(dataset)\n",
    "    data = np.delete(data, index, axis=0)\n",
    "    # Resize the dataset and overwrite with the new data\n",
    "    dataset.resize(data.shape)\n",
    "    dataset[...] = data\n",
    "    shape_after = dataset.shape; \n",
    "    print(f\"Successfully Delete the entry {index}; Shape: {shape_before} -> {shape_after}\")\n",
    "\n",
    "  def create_dataset(self, data_key, thedata, columns=[], **kwarg): \n",
    "    theshape = thedata.shape; \n",
    "    maxshape = [i for i in theshape]; \n",
    "    maxshape[0] = None; \n",
    "    maxshape = tuple(maxshape); \n",
    "    dset = self.hdffile.create_dataset(data_key, data=thedata, compression=\"gzip\", maxshape=maxshape, **kwarg)\n",
    "    print(f\"Created Dataset: {data_key}\"); \n",
    "    return dset\n",
    "  \n",
    "  def create_table(self, data_key, thedata, columns=[], **kwarg): \n",
    "    if thedata.dtype.names: \n",
    "      names = [name.encode('utf-8') for name in thedata.dtype.names]\n",
    "    else: \n",
    "      names = columns; \n",
    "    self.create_dataset(data_key, thedata, **kwarg); \n",
    "    if len(columns) > 0: \n",
    "      dset.attrs['columns'] = names\n",
    "      \n",
    "  def delete_dataset(self, dataset_name):\n",
    "    if dataset_name in self.hdffile:\n",
    "      self.hdffile.pop(dataset_name, None); \n",
    "      print(f\"Dataset '{dataset_name}' has been deleted.\")\n",
    "    else:\n",
    "      print(f\"Warning: Dataset '{dataset_name}' does not exist.\")\n",
    "\n",
    "  \n",
    "  def append_entry(self, dataset_name, newdata):\n",
    "    dset = self.data(dataset_name);\n",
    "    current_shape = dset.shape;\n",
    "    # Calculate the new shape after appending the new data\n",
    "    new_shape = (current_shape[0] + newdata.shape[0], *current_shape[1:])\n",
    "    # Resize the dataset to accommodate the new data\n",
    "    dset.resize(new_shape)\n",
    "    # Append the new data to the dataset\n",
    "    dset[-newdata.shape[0]:] = newdata\n",
    "    print(f\"Appended {newdata.shape[0]} entries to {dataset_name}\")\n",
    "    \n",
    "  def draw_structure(self):\n",
    "    print(\"####### HDF File Structure #######\")\n",
    "    def print_structure(name, obj):\n",
    "      if isinstance(obj, h5.Group):\n",
    "        print(f\"$ /{name:10s}/\")\n",
    "      else:\n",
    "        print(f\"$ /{name:10s}: Shape-{obj.shape}\")\n",
    "    self.hdffile.visititems(print_structure)\n",
    "    print(\"##### END HDF File Structure #####\")\n",
    "    \n",
    "  def alter_entry(self, dataset_name, index, new_data):\n",
    "    \"\"\"\n",
    "    Alter a specific entry in the dataset by index.\n",
    "    Args:\n",
    "      dataset_name (str): The name of the dataset.\n",
    "      index (int): The index of the entry to be modified.\n",
    "      new_data (tuple): The new data to replace the existing entry.\n",
    "    \"\"\"\n",
    "    dataset = self.data(dataset_name)\n",
    "    if 0 <= index < len(dataset):\n",
    "      dataset[index] = new_data\n",
    "    else:\n",
    "      print(f\"Index {index} is out of range for dataset '{dataset_name}'.\")\n",
    "    \n",
    "  \n",
    "print(\"################\")\n",
    "hdf = io.hdf_operator(\"/tmp/mytestfile.h5\")\n",
    "hdf.create_table(\"test\", data, columns=[])\n",
    "hdf.create_table(\"test2\", matrix)\n",
    "\n",
    "# Entry Addition\n",
    "hdf.append_entry(\"train_x\", matrix)\n",
    "\n",
    "# Entry Deletion\n",
    "hdf.mask_entries(\"train_x\", np.random.random(size=200)>0.1)\n",
    "hdf.remove_entry(\"train_x\", 10)\n",
    "\n",
    "# Entry Alternation\n",
    "hdf.alter_entry(\"train_x\",1, (np.zeros((32,32,32))+1.5).astype(float))\n",
    "\n",
    "\n",
    "hdf.append_entry(\"test\", data)\n",
    "hdf.append_entry(\"test\", data)\n",
    "\n",
    "hdf.column_names(\"test\")\n",
    "x = hdf.data(\"test\")\n",
    "hdf.draw_structure()\n",
    "\n",
    "hdf.delete_dataset(\"test2\")\n",
    "\n",
    "hdf.draw_structure()\n",
    "\n",
    "datasets = hdf.list_datasets()\n",
    "names = hdf.column_names(\"train_x\")\n",
    "print(names)\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d0d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        15\n"
     ]
    }
   ],
   "source": [
    "print(f\"{15:10d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74680e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bf824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0518f918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'col1', b'col2', b'col3']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Create a structured array\n",
    "data = np.array([(1, 1.1, np.arange(100).reshape((10,10))), (2, 2.2, np.arange(100,200).reshape((10,10)))], \n",
    "                dtype=[('col1', 'i4'), ('col2', 'f4'), ('col3', h5py.vlen_dtype(np.dtype('float64')))])\n",
    "\n",
    "# Create an HDF5 file\n",
    "with h5py.File('/tmp/example.h5', 'w') as f:\n",
    "    # Create a dataset with the structured array\n",
    "    dset = f.create_dataset('table', data=data)\n",
    "\n",
    "    # Store the column names as an attribute of the dataset\n",
    "    column_names = [name.encode('utf-8') for name in data.dtype.names]\n",
    "    print(column_names)\n",
    "    dset.attrs['column_names'] = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e2b07a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5\u001b[49m\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/mytestfile.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file1: \n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(file1\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(file1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_x\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5' is not defined"
     ]
    }
   ],
   "source": [
    "with h5.File(\"/tmp/mytestfile.h5\",\"r\") as file1: \n",
    "  print(file1.keys())\n",
    "  print(file1[\"train_x\"])\n",
    "  x = list(file1[\"train_x\"])\n",
    "  print(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2650711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5987bf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bool_(np.arange(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e049a30",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert between src and dst data types (no appropriate function for conversion path)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m dt \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mspecial_dtype(vlen\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype((\u001b[38;5;28mfloat\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))))\n\u001b[1;32m     34\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1.1\u001b[39m, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2.2\u001b[39m, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m)))],\n\u001b[1;32m     35\u001b[0m                 dtype\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi4\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol3\u001b[39m\u001b[38;5;124m'\u001b[39m, dt)])\n\u001b[0;32m---> 36\u001b[0m \u001b[43mhdf_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m hdf_op\u001b[38;5;241m.\u001b[39mhdffile\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mhdf_operator.create_dataset\u001b[0;34m(self, dataset_name, data, dtype)\u001b[0m\n\u001b[1;32m     24\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi4\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol3\u001b[39m\u001b[38;5;124m'\u001b[39m, dt)])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Create the dataset\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdffile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlprod2/lib/python3.8/site-packages/h5py/_hl/group.py:136\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    133\u001b[0m     kwds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_order\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mget_config()\u001b[38;5;241m.\u001b[39mtrack_order\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[0;32m--> 136\u001b[0m     dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlprod2/lib/python3.8/site-packages/h5py/_hl/dataset.py:170\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl)\u001b[0m\n\u001b[1;32m    167\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m h5d\u001b[38;5;241m.\u001b[39mcreate(parent\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28;01mNone\u001b[39;00m, tid, sid, dcpl\u001b[38;5;241m=\u001b[39mdcpl)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mdset_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:221\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_proxy.pyx:163\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/defs.pyx:3536\u001b[0m, in \u001b[0;36mh5py.defs.H5Tconvert\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_conv.pyx:778\u001b[0m, in \u001b[0;36mh5py._conv.ndarray2vlen\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_conv.pyx:808\u001b[0m, in \u001b[0;36mh5py._conv.conv_ndarray2vlen\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to convert between src and dst data types (no appropriate function for conversion path)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "\n",
    "class hdf_operator:\n",
    "    def __init__(self, filename, new=False):\n",
    "        if new or (not os.path.isfile(filename)):\n",
    "            self.hdffile = h5.File(filename, \"w\")\n",
    "        else:\n",
    "            self.hdffile = h5.File(filename, \"a\")\n",
    "\n",
    "    def create_dataset(self, dataset_name, data, dtype=None):\n",
    "        \"\"\"\n",
    "        Create a new dataset with the given data.\n",
    "\n",
    "        Args:\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        data (array-like): The data to store in the dataset.\n",
    "        dtype (optional): The data type of the dataset.\n",
    "        \"\"\"\n",
    "        # Create a compound data type if not provided\n",
    "        if dtype is None:\n",
    "            dt = h5.special_dtype(vlen=np.dtype((float, (10, 10))))\n",
    "            dtype = np.dtype([('col1', 'i4'), ('col2', 'f4'), ('col3', dt)])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = self.hdffile.create_dataset(dataset_name, data=data, dtype=dtype, maxshape=(None,) + data.shape[1:])\n",
    "\n",
    "    # ... (previous methods)\n",
    "\n",
    "# Usage example\n",
    "hdf_op = hdf_operator(\"/tmp/mytestfile.h5\", new=True)\n",
    "dt = h5.special_dtype(vlen=np.dtype((float, (10, 10))))\n",
    "data = np.array([(1, 1.1, np.arange(100).reshape((10, 10))), (2, 2.2, np.arange(100, 200).reshape((10, 10)))],\n",
    "                dtype=[('col1', 'i4'), ('col2', 'f4'), ('col3', dt)])\n",
    "hdf_op.create_dataset(\"my_dataset\", data)\n",
    "hdf_op.hdffile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c47d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
